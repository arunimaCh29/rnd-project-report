%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
    \section{Introduction}
    \label{sec:introduction}

    % This is a template for MAS R\&D projects, based on \emph{IEEETran}.
    % Here are some preliminaries about some common things you need to do to use the template:
    % \begin{itemize}
    %     \item Add your references to the file \emph{references.bib} and cite them as Mustermann and Smith \cite{referenceexample} (if there are more than three authors, cite as Mustermann et al. \cite{referenceexample}).
    %     \item Refer to sections as Sec. \ref{sec:introduction}.
    %     \item You can include figures as follows (note that the figure caption is below the figure).
    %     \begin{figure}[ht]
    %         \centering
    %         \includegraphics[width=0.8\linewidth]{figures/b-it-logo.pdf}
    %         \caption{My caption}
    %         \label{fig:figureexample}
    %     \end{figure}
    %     Refer to figures as Fig. \ref{fig:figureexample}.
    %     \item You can add tables as follows (note that the table caption is above the table).
    %     \begin{table}[ht]
    %         \caption{My caption}
    %         \label{tab:tableexample}
    %         \begin{tabular}{M{0.45\linewidth} M{0.45\linewidth}}
    %             \hline
    %             \cellcolor{gray!10!white} Header 1 & \cellcolor{gray!10!white} Header 2 \\\hline
    %             Cell 1 & Cell 2 \\\hline
    %             Cell 3 & Cell 4 \\\hline
    %         \end{tabular}
    %     \end{table}
    %     Refer to tables as Tab. \ref{tab:tableexample}.
    %     \item You can add equations as follows.
    %     \begin{equation}
    %         f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left( \frac{x - \mu}{\sigma} \right)^2}
    %         \label{eq:equationexample}
    %     \end{equation}
    %     Refer to equations as Eq. \ref{eq:equationexample}.
    % \end{itemize}
    The set of methodologies and techniques when applied to AI systems, helping in interpretations of the decisions made by the system are called Explainable Artificial Intelligence (XAI) methods. With the increase in use of deep neural network based Automatic Speech Recognition (ASR) systems in our day-to-day life, the need has arisen for a certain amount of trustworthiness for the user to accept the results generated by the system \cite{doi:10.34133/icomputing.0074}. Current ASR systems often operate as ``black boxes'' with limited insights into their decision-making processes. The state of the art ASR systems are transformer-based models that are increasingly outperforming ASR systems based on classical approaches like Hidden Markov Models \cite{10143311}. Most of the research on explainability has been concentrated on visual and structured data. There is a lack of research for Explainable AI specifically in ASR \cite{doi:10.34133/icomputing.0074,10094635,10143311}. This research addresses the mentioned gap by exploring adaptibility of visual domain interpretability methods for interpretability of transformer-based ASR models.

    For the ASR model, we focus on transfomer-based end-to-end ASR models, specifically the Whisper \cite{radford2023robust} model. End-to-end models use a single neural network to map audio inputs directly to text outputs. These models typically use deep learning architectures that jointly learn acoustic, pronunciation, and language representations.

    In this research, we adapt one such method AtMan \cite{NEURIPS2023_c83bc020} as our explainability method to adapt for the Whisper model \cite{radford2023robust}. Originally developed for vision transformers and natural language processing tasks, AtMan (Attention Manipulation) is a novel approach that leverages attention manipulation techniques to provide insights into transformer model decisions. It works by systematically introducing perturbations through attention manipulations across different transformer layers during forward propagation, helping to reveal the model's internal decision-making process. The method was initially demonstrated on transformer based language models for image classification through prompt and text generation tasks, where it successfully identified relevancy of tokens for model predictions. We adapt the method for end-to-end ASR Whisper model and evaluate it on Librispeech \cite{panayotov2015librispeech} dataset, examining the relevance of particular time sequence for the output tokens generated by the model.

    \subsection{Motivation}
    \label{sec:introduction:motivation}

    The motivation for this research is to provide a method to interpret the decisions made by transformer-based ASR models. This is crucial for several reasons. First, as ASR systems become increasingly integrated into critical applications like healthcare, legal transcription, and emergency response systems \cite{doi:10.34133/icomputing.0074}, understanding their decision-making process becomes vital for ensuring reliability and accountability. Second, in cases where ASR systems make errors, having interpretable models allows developers to identify the root causes of these mistakes, whether they stem from acoustic ambiguities, background noise, or linguistic complexities. Third, explainability builds trust with end-users by providing transparency about how their speech is being processed and transcribed, particularly important in privacy-sensitive contexts. Additionally, for developers and researchers, understanding the internal workings of these models is essential for systematic improvement and debugging of ASR systems \cite{10143311}, as well as for ensuring compliance with emerging AI regulations that may require algorithmic transparency.

    \subsection{Problem Statement}
    \label{sec:introduction:problem_statement}

    This research investigates the adaptation of successful XAI methods from the visual, structured, and audio domains, which have not yet been applied to ASR systems, to improve accountability and transparency. The primary objective is to assess the feasibility of directly adapting these methods for ASR explainability. This investigation will focus on identifying the necessary modifications for direct adaptation and evaluating the relevance and effectiveness of the resulting explanations.
    \begin{itemize}
        \item Adapting visual domain interpretability methods, particularly AtMan \cite{NEURIPS2023_c83bc020}, for ASR applications.
        \item Understanding how audio input in relation to attention mechanisms in transformer-based ASR models influence transcription decisions.
    \end{itemize}

    \subsection{Proposed Approach}
    \label{sec:introduction:proposed_approach}

    Upon reviewing existing methods, we identified AtMan \cite{NEURIPS2023_c83bc020} as a novel technique that examines model behavior by introducing controlled perturbations to attention weights within transformer layers. By systematically altering attention patterns and observing their effects on the model's predictions, AtMan reveals which tokens, when suppressed, have the greatest impact on subsequent tokens, or which previous tokens most significantly influence a particular token. This provides valuable insights into the model's decision-making process. We apply this method to elucidate the decisions made by the Whisper model \cite{radford2023robust} on the Librispeech \cite{panayotov2015librispeech} dataset. By perturbing the attention weights in the encoder layers and observing the resulting changes in the model's predictions, we aim to identify the audio segments that are most critical to the model's decision-making process.

\end{document}
