%!TEX root = ../report.tex
\documentclass[../report.tex]{subfiles}

\begin{document}
    \section{Related Work}
    \label{sec:related_work}

    % Summarise the relevant related work in this section and position your work with respect to the related work.

    \subsection{Explainability in Audio Domain}
    According to Akman and Schuller \cite{doi:10.34133/icomputing.0074}, audio explainability methods can be categorized into two groups: generic methods adapted for audio \cite{bach2015pixel,Selvaraju_2019,lundberg2017unified} and specialized methods designed specifically for audio models \cite{10094635,10143311,10447390,haunschmid2020audiolime}. In AudioLIME, Haunschmid et al. \cite{haunschmid2020audiolime} adapted LIME \cite{ribeiro2016should} for a music tagging system. It uses source separation to provide listenable explanations, enabling users to understand the model's decisions by hearing the separated components that influenced the decision most. 

    In the context of speech processing, Sivasankaran et al. \cite{sivasankaran2021explaining} applied DeepSHAP to speech enhancement models, revealing which frequency-time bins in the input spectrogram influence the model's masking decisions.

    Frommholz et al. \cite{10.1145/3617233.3617265} utilized the post-hoc interpretability method LRP in conjunction with the Discrete Fourier Transform (DFT) to explore how various input representations influence the decision-making process of the model. They compared the relevance of heatmaps from models trained on different input forms to assess how these representations affect the model's output.

    Recent work by Parekh et al. \cite{10413597} has also explored the use of Non-negative Matrix Factorization (NMF) for interpretability in audio classification networks, providing a novel approach to understanding model decisions through decomposition of audio features.

    \subsection{Explainability in ASR}
    The field of explainability in ASR was pioneered by Wu, Bell and Rajan \cite{10094635} with the X-ASR framework. The authors adapted generic XAI methods to ASR models, namely Statistical Fault Localization (SFL) \cite{sun2020explaining}, Local Interpretable Model-Agnostic Explanations (LIME) \cite{ribeiro2016should} and causal explanations \cite{chockler2021explanations}. They emphasize on the importance of qualitative assessment of the explanations, evaluated by the size and consistency of explanations of transcriptions generated across various ASR models for the respective methods. Wu, Bell and Rajan \cite{wu2024can} uses state of the art visual domain XAI method Local Interpretable Model-Agnostic Explanations (LIME), adapted from the image classification domain to ASR model for phoneme recognition and demonstrates reliable explanations 96\% of the time in its top three audio-segments in a controlled evaluation setting.
     A significant advancement came with GRAD-SAS by Sun et al. \cite{10143311}, which adapted Grad-CAM \cite{Selvaraju_2019} for speech recognition transformers. This technique involves calculating gradients by deriving classification scores and combining attention matrices with their gradients through inner-product and summation operations to visualize the weighted attention on input tokens, providing insights into the model's focus during transcription.

    Recent research by Javadi et al. \cite{10669910} has focused on understanding word-level relationships in ASR outputs. They proposed a novel approach for word-level error ASR quality estimation by analyzing attention patterns in reference-free metrics. This method not only helps in understanding the relationship between input audio and transcribed words but also enables efficient corpus sampling and post-editing. The approach demonstrates how attention based error analysis can be leveraged to provide explanations at the word level.

    \subsection{Explainability in Audio Transformers}
    Explainability in Audio Transformers is crucial as they outperform other deep neural networks for audio-specific tasks as shown by Prabhavalkar et al. \cite{10301513}.
    AttHear by Akman and Schuller \cite{10447390} explains audio transformers by generating listenable explanations using inverse Short-time Fourier Transform (STFT). It pre-processes the audio to extract features, uses a pre-trained transformer to compute attention scores, and applies Non-negative Matrix Factorization (NMF) weighted by these scores to identify the important audio parts. Bicer et al. \cite{9914699} enhanced an acoustic scene classification model's interpretability using Grad-CAM and guided backpropagation, providing visual insights into which parts of the input contributed most to the model's decisions.
    As for the application of explainability in Audio domain, Vitale et al. \cite{vitale2024exploring} utilizes explainability techniques to analyze emergent syllables in end-to-end ASR systems. They used model explainability techniques to investigate how these recognizers identify and process syllables, providing insights into the model's internal mechanisms and improving our understanding of how speech patterns are learned and recognized by these advanced models. Research by Singla et al. \cite{10031189} has also explored probing transformer representations for language delivery and structure, providing insights into how these models process and understand speech patterns. This work complements traditional explainability approaches by revealing the internal representations learned by transformer models and their relationship to linguistic features.

    \subsection{Explainability in Visual Transformers}
    Our research focuses on adapting visual domain methods for ASR, particularly using end-to-end transformer models. Therefore, understanding methods in visual transformers is essential. These methods are categorized into feature-attribution, attention-based, pruning-based, inherently explainable methods, and other tasks as described by Kashefi et al. \cite{kashefi2023explainability}. Feature-attribution techniques like Grad-CAM \cite{Selvaraju_2019} and Layer-wise Relevance Propagation (LRP) \cite{bach2015pixel} are most commonly utilized methods in the domain. Grad-CAM generates heatmaps by utilizing model gradients to highlight class-relevant areas in input images, thereby improving model transparency. The approach by Chefer et al. Chefer, Gur and Wolf \cite{9710570} offers a generic attention-model explainability method for bi-modal and encoder-decoder transformers, focusing on how input components influence model decisions through attention mechanisms. T-TAME by Ntrougkas, Gkalelis and Mezaris \cite{10539635} is another attention-based method that employs unsupervised learning to train attention mechanisms using feature maps, facilitating the interpretation of model processing and feature prioritization. R-Cut by Niu et al. Niu et al. \cite{niu2024r} creates alternative activation maps from normalization layers, perturbs input images, and uses similarity scores from a pre-trained model to weigh these maps. It uses class-aware patch tokens to form a weighted graph, where nodes represent semantic features, and generates explainability maps by partitioning with graph eigenvectors, enhancing class-specific decision understanding. LeGrad by Bousselham et al. \cite{bousselham2024legrad} utilizes the self-attention mechanism in ViT \cite{dosovitskiy2020image} by computing gradients concerning attention at each layer, summing up explainability maps from each layer to produce a final map.
    Ali et al. \cite{pmlr-v162-ali22a} demonstrate that traditional XAI methods often produce unreliable results when applied to transformer models, and propose an extension to Layer-wise Relevance Propagation (LRP) \cite{bach2015pixel}. Their enhanced method introduces conservative propagation rules for transformer architectures, resulting in more consistent and reliable explanations across different model components.
    For transformers, most methods leverage attention mechanisms. AtMan by Deiseroth et al. \cite{NEURIPS2023_c83bc020} proposes understanding transformer predictions through memory-efficient attention manipulation.
    It is essentially developed for generative transformers that explore how different parts of the input affect the model's predictions. 
    It manipulates attention mechanisms by perturbing token embeddings and measures changes in model output. By comparing these perturbed embeddings using cosine similarity, AtMan \cite{NEURIPS2023_c83bc020} identifies which input tokens are crucial for the model's decisions without relying on traditional backpropagation methods. 

    Our research enhances the existing body of work on explainability for ASR and audio transformers by adapting and expanding the AtMan method by Deiseroth et al. \cite{NEURIPS2023_c83bc020}, which was initially created for visual transformers, to the ASR domain. Previous studies such as X-ASR by Wu, Bell and Rajan et al. \cite{10094635} and GRAD-SAS by Sun et al. \cite{10143311} have established the groundwork for ASR explainability. However, our method emphasizes manipulating attention in the embedding space to gain deeper insights into transformer-based ASR models, while avoiding the computationally intensive backpropagation typically used in attention-based explainability techniques.

    Unlike traditional methods that rely on gradient-based explanations or post-hoc analysis, our adaptation of AtMan systematically perturbs attention weights within the transformer layers to reveal the internal decision-making process of the model. This approach allows us to identify the most critical audio segments influencing the model's predictions, thereby enhancing the interpretability of end-to-end ASR systems.






\end{document}
